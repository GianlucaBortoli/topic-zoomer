\documentclass{sig-alternate-05-2015}
\usepackage{subscript}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}

\graphicspath{ {./imgs/} }

\begin{document}

\title{Topic-zoomer: a URL categorization system}

\numberofauthors{2}
\author{
    \alignauthor
    Gianluca Bortoli\\
           \affaddr{DISI\,-\,University of Trento}\\
           \affaddr{Student id: 179816}\\
           \email{gianluca.bortoli@studenti.unitn.it}
    \alignauthor
    Pierfrancesco Ardino\\
           \affaddr{DISI\,-\,University of Trento}\\
           \affaddr{Student id: 189159}\\
           \email{pierfrancesco.ardino@studenti.unitn.it}
    }
\maketitle


\begin{abstract}
Smartphones and tablets pervade our lives on a daily basis and people use them in most curious ways.\\ 
The key idea behind this work is to take advantage of the traffic they generate while they are connected to the Internet, extracting \emph{topics} from what people search on the web with their mobile devices. This use case provides also the geographical coordinates of the users based on the telephone cell they are connected to.\\
For the purpose of this work the approximate location of the cell is enough, even thoug it implicitly defines a limit on the precision while selecting an area inside a map.\\
This paper presents Topic-zoomer, a URL categorization system built upon Apache Spark.
\end{abstract}


\printccsdesc
\keywords{Big data, Data mining, URL categorization, topic extraction, geolocalized URLs, Latent Dirichlet Allocation}


\section{Introduction}
Nowadays extracting topics from a text is a very common goal in Big Data and the problem of categorizing URLs is highly connected to it.\\
Generally speaking, the main purpose is to find what a set of texts is talking about. Applying this concept to a more specific problem, Topic-zoomer is able to find the topics on a subset of the initial dataset, based on the documents' geographical location.\\
The tool starts from a set of geotagged URLs and is able to restrict its search space only to the web pages lying inside a certain geographical region. In this way, it can discover what the pages in that specific area are talking about.\\
This work can be employed in many different fields. For example, it can be utilized to see what people are going to do in the future basing on their web searches. Moreover, it can be useful for public institutions such as municipalities; they could understand the citizens' needs in order to improve the functionality of the city's facilities and their territory coverage.\\
However, the main problem of a real-life usage of this kind of analysis can be the data source. Internet Service Providers (ISPs) can easily extract and provide geotagged URLs based on the HTTP requests passing through their routers. Furthermore, the anonymity of the users generating such traffic must be preserved.\\
This kind of problems is a challenge for traditional programming paradigms, since the amount of data can be huge and it may also involve streaming  processing. Big Data platforms and frameworks perfectly suit this amount of work that cannot fit in a single machine allowing to scale as the input size increases. Thus employing an engine for large-scale and distributed data processing is certainly not an option if a system is thought to be used on real data.


\section{Related work}
URL categorization is a task that many people and companies are addressing in the last few years, pushing even to the categorization of the whole World Wild Web. Some of the companies involved in this kind of analysis are Cyren\footnote{\url{http://www.cyren.com/url-category-check.html}} and BrightCloud\footnote{\url{https://www.brightcloud.com/tools/change-request-url-categorization.php}}. Moreover, a huge number of academic researches have been published and also some patents\footnote{\url{https://www.google.com/patents/US8078625}} have been filed on the subject. However, the specific case of geotagged URLs has not raised so much interest neither in the academic nor in the developer communities.\\
Furthermore, when approaching problems related to Big Data, the challange is to deal with huge amounts of data that cannot fit in a single machine. This implies that merely sequential solutions are not admitted, since they cannot handle such input size and are able to run the computation only on a single machine. Thus a distributed data processing framework is the only feasible way to solve these kind of problems and achieve scalability\cite{scalability_performances}.\\
Nowadays the most known and used platforms are Apache Hadoop and Apache Spark. The core part of Topic-zoomer makes use of the latter, since Spark is provably faster\cite{evaluation, evaluation2}. This performance gap is mainly due to the in-memory nature of Spark and its acyclic data flow among the worker nodes. Moreover, this work makes use of Spark DataFrames rather than RDDs for multiple reasons. First, a DataFrame has a tabular structure that allows the Spark engine to run a lot of optimizations on the queries. Moreover, it allows sorting and shuffling operations without deserializing the data structure. Finally the DataFrame's API allows to build relational query plans\cite{relational_query} that the Spark Catalyst optimizer is able to execute.  

\section{Problem definition}
The task Topic-zoomer aims to solve can be better described starting from its \emph{input} and \emph{output} data.\\
The \emph{input} is a CSV file containing rows of the following form:
\begin{equation}\label{dataset}
    <latitude,\,longitude,\,\{url0\,|url1\,|...\,\}\,>
\end{equation}
The text of the pages pointed by the URLs is analized in order to identify the topics that are represented as a vector of words that very often appear togheter.\\
Let \emph{A} be an algorithm to compute such topics (eg. TF/IDF, frequent itemset or LDA; see Section \ref{algorithmSelection} for a more comprehensive description), the algorithm \emph{L} gives as a result the topics of the dataset in general.\\
\emph{L} also allows the user to restrict the search in the dataset using the following parameters:
\begin{itemize}
    \item \emph{A}: the map area identified by top-left and bottom-right corners. This is the region of interest for retrieving the topics. 
    \item \emph{S}: the size of the grid to be created inside the area \emph{A} (ie. the square side length).
    \item \emph{k}: the number of topics to search inside each square of the grid.
\end{itemize}
In this way \emph{A} is divided into squares of size $S \times S$ and the \emph{k} topics are identified inside each square.


\section{Solution}
This section describes in detail all the steps needed to realize this tool, from the initial web page download to the map-reduce operations taken by Spark. The order by which they are presented reflects the one that was followed during the experiments. Furthermore, Figure \ref{solution} represents the architecture of the overall solution.
\begin{figure*}
  \center{\includegraphics[width=0.55\textwidth]{solution.png}}
  \caption{The schema representing the overall solution.}
  \label{solution}
\end{figure*}

\subsection{Parallelized and sequential versions}
As stated before, when dealing with real Big Data problems using an engine for large-scale data processing is a must. However, if the input size is not large enough, these systems exploiting the map-reduce paradigm may not be the most performing solution. This is due to the fact that they introduce some overhead in the computation, since the workload has to be split into multiple tasks to be assigned to the workers, the input data for the computation needs to be sent to them and only after the real computation the final result can be retrieved.\\
This architecture, which is nearly invisible to the programmer, clearly introduces some delay with respect to a classic sequential solution. On the other hand, when the size of the data cannot fit in a single machine, a distributed computing system is the only feasible way to solve such problems. Thus, only products like Spark allow to process this huge amount of information and achieve scalability.
\subsection{Data collection and preprocessing}\label{preprocessing}
The first task was to transform an initial dataset with the form shown in \ref{dataset} into another one of the form
\begin{equation}\label{datasetClean}
    <latitude,\,longitude,\,page\_text\,>
\end{equation}
A \emph{crawler} is written in order to follow all the links and download the web pages. This may seem a trivial operation at a first glance. However, the initial dataset contained a lot of URLs pointing to images, videos, configuration files for mobile phones applications and many other elements that are far from being a text web page. To retrieve only sensible information, the crawler rejects all the requests not containing \emph{text/html} in the responses' header. This is the one which is usually populated by a web server when a client visits one of its pages. Last but not least, the crawler uses multiple processe to minimize the download time. Multiple processes are preferred to multiple threads because the crawler is written in Python and this language is known to be subject to the Global Interpreter Lock\cite{gil} (GIL).\\
Finally, a new CSV file containing rows as described in \ref{datasetClean} is generated.
\subsection{Algorithm selection}\label{algorithmSelection}
After downloading the web pages, the Spark pipeline is ready to run on the clean dataset.\\
Among all the ways to extract topic, the \emph{Latent Dirichlet Allocation}\cite{lda} (LDA) algorithm is chosen as \emph{L} to find the topics of the dataset in general. LDA is a generative and probabilistic model for collections of discrite data such as text corpora. It is a Bayesian hierarchical model, where each topic is modeled over an underlying set of topic probabilities. LDA is a good candidate when choosing between different methods to classify text. This work uses the implementation inside the Spark MLlib for the inference step.\\
From a higher perspective LDA can be seen as a clustering technique, whose goal is to divide the document corpora given as input into \emph{k} clusters, which are the topics to be retrieved from the text.
\subsection{Spark pipeline}
When dealing with Big Data problems like the one discussed in this paper, it is mandatory to use a map\-reduce approach in order to be able reach a solution. Nevertheless, in order to solve it fast some kind of parallelization and workload optimization is needed. For this reason the multi\-step parallelized procedure following the map-reduce pattern is written using the Spark framework. This tool also allows to write applications that scales as the input size increases.\\
The Python\footnote{For the seek of completeness, Python3 with version \textless\,3.6 is used.} language is chosen for the development of this work in order to focus on the real data manipulation and analysis rather than continuously dealing with language-specific issues. Moreover, PySpark, the Python interface for the Spark, exposes the Spark programming model to the developer. Nevertheless, the APIs are very concise and makes the application maintainable in long-term projects.\\
The pipeline of operations can be better visualized in Figure \ref{spark_pipeline} and it is composed as follows:
\begin{enumerate}
    \item load the dataset (in CSV format) generated by the pre-processing phase described in Section \ref{preprocessing} into the HDFS.
    \item provide Topic-zoomer with the parameters \emph{A}, \emph{S} and \emph{k}.
    \item divide \emph{A} into squares of dimension $S \times S$ (ie. compute the inner grid).
    \item filter the input data to retrieve only the points lying inside each square of the grid. At the same time a dataframe for each one of them is created.
    \item for each square remove punctuation and stopwords\cite{mmd} for the language of interest\footnote{In this specific case, Italian, English and German stopwords are removed due to the source of the dataset used for the experiments.} in order not to consider the most common short function words.
    \item convert every text into vectors of token counts. This step is needed to prepare the dataframes with a format suitable for the LDA algorithm.
    \item train the LDA model with the proper \emph{k} parameter.
    \item collect the results from HDFS.
\end{enumerate}

\begin{figure*}
  \center{\includegraphics[width=0.8\textwidth]{spark_pipeline.png}}
  \caption{All the steps of the Spark pipeline represented in detail.}
  \label{spark_pipeline}
\end{figure*}

The choice of creating one Spark DataFrame for each $S \times S$ square in the grid allows to fully exploit the parallelism and the workload optimization offered by Spark. In this way, the master node always has data to send to the workers to keep them busy until the end of the computation. This approach should optimize the CPU usage in the worker nodes, taking advantage of the full computation power made available by the entire cluster.\\
Finally, the whole pipeline is written having in mind how Spark deals with the data and spreads it among all the worker nodes. Thus, the map\-reduce operations are organized in a way to minimize also the amount of data that needs to be exchanged between the nodes through the network. Even though the network physically connecting the machines used in the experiments displayed in Section \ref{experiments} is 2Gbps per core\footnote{\url{https://cloud.google.com/compute/docs/networks-and-firewalls}}, paying special attention to this aspect can make a huge difference from the overall performance standpoint. For this reason, to furtherly reduce the amount of intermediate representations inside the Spark pipeline, Topic-zoomer keeps only the bare essential information from the previous step inside each DataFrame. In this way it is possile to optimize not only how data flaws between the nodes, but also to reduce its size and thus minimize the network overhead.
\subsection{Avoiding recomputation}\label{avoidingRecomputation}
One of the main issues these kind of tools have to face is the overall time to answer a user query.\\
The main idea behind this feature can be better explained referring to Figure \ref{recomputationSchema}. The top-left square represents an area (\emph{oldA}) already analyzed by the tool in a previous run, while the bottom-right one stands for a new area (\emph{newA}) of interest. It would be useful not to recompute the topics for the whole \emph{newA}, since \emph{oldA} and \emph{newA} have a part in common (ie. \emph{commonA}, the blue square). This can be achieved training the LDA model only on the red area without including the \emph{commonA} part, as it would happen without this feature. As the blue region becomes bigger, the whole run of the tool takes less time with respect to a na\"{i}ve implementation. This improvement is due to the fact that \emph{newA} contains less points and thus the underlying LDA model is trained with fewer items.\\
Nevertheless, this functionality is useful on multiple situations:
\begin{itemize}
    \item the area \emph{A} selected remains unchanged while the size of the step \emph{S} is reduced or increased. This happens when a user wants to change the size of the grid.
    \item the size of the step \emph{S} remains the same while the selected area \emph{A} is moved. This happens when a user moves the view to another part of the map to better focus on some other specific place.
    \item both \emph{A} and \emph{S} are changed. 
\end{itemize}
In this way, this method is effective in every possible change in the user input.\\
Dealing with the scenarios mentioned above, the overall result returned for \emph{newA} must take the topics coming from \emph{commonA} into account. The LDA returns as output a list containing the topics in the following form:
\begin{equation}
    \begin{split}
        [\,probability_0,\,topic_0\,] \\
        \cdots \\
        [\,probability_k,\,topic_k\,]
    \end{split}
\end{equation}
The lists computed starting both from \emph{newA} and \emph{commonA} have to be merged, producing a single topic lisk of length \emph{k} as output. The merging procedure is implemented as follows; the list produced starting from \emph{newA} is considered as is, while the probability values in the list produced starting from \emph{oldA} are multiplyed by a factor \emph{f} computed as shown in Equation \ref{factor}.
\begin{equation}\label{factor}
    f = \frac{area_{commonA}}{area_{oldA}}
\end{equation}
In this way \emph{f} weights the topics previously found for \emph{oldA} with respect to the size of \emph{commonA} when computing the tipics starting from \emph{newA}. To retrieve the final output all the topics are ordered by decreasing probability values and  the first \emph{k} of them are taken.

\begin{figure}[t]
  \center{\includegraphics[width=0.22\textwidth]{recomputationSchema.png}}
  \caption{A scenario where avoiding useless recomputation is applicable and useful.}
  \label{recomputationSchema}
\end{figure}

\section{Architecture}
Apache Spark and all the other big data distributed frameworks are thought to be set up in a cluster of machines. In this way it is possible to exploit its distributed mode and all the computation power from the worker nodes. To achive this, Google Dataproc\footnote{\url{https://cloud.google.com/dataproc/}} on the Google Compute Engine\footnote{\url{https://cloud.google.com/compute/}} (GCE) is used to provision and setup a Spark cluster.\\

One of the main focuses of this work is the \emph{scalability} of this piece of software. To see if Topic-zoomer really succeedes in it, a pseudo-distributed Spark cluster may produce completely misleading or even wrong results. Thus a platform providing highly performing vistual machines (VMs) is mandatory.\\
The cluster used for the experiments described in Section \ref{experiments} is composed as shown in Table \ref{cluster}. Google Dataproc uses Apache YARN and provides access to an Hadoop Distributed File System (HDFS) shared among all the nodes.\\
Moreover, it is fully integrated with Google Cloud Storage (GCS), a flexible, scalable and distributed storage for VM instances. All the input data presented in Table \ref{datasets} are stored in a shared bucket on GCS to allow every node in the cluster to mount it through its FUSE\footnote{\url{https://www.kernel.org/doc/Documentation/filesystems/fuse.txt}} adapter.  

\begin{table}[]
    \centering
    \caption{GCE cluster node characteristics.}
    \label{cluster}
    \begin{tabular}{lllll}
    \hline
    Node   & Number & vCPUs & RAM (GB) & SSD (GB) \\
    \hline
    master & 1      & 1     & 3.75     & 500      \\
    worker & 3      & 2     & 7.50     & 500     
    \end{tabular}
\end{table}


\section{Experiments}\label{experiments}
The experiments shown in this section focus on an analysis of the quantitative rather than the qualitative performances of the system. Measuring time is a way to provide unbiased results regarding its real ability to scale and respond to queries.\\
Some performance measures have to be computed in order to understand how Topic-zoomer behaves under different scenarios. In order to better understand the results, Table \ref{datasets} must be taken as reference. It displays the composition and the sizes of the input data used throughout all the experiments. As it is possible to see, the whole dataset (i.e. \emph{input\_100}) is split into different parts with increasing size in order to be able to run the experiments with different volumes of data.

\begin{table}[]
    \centering
    \caption{Input data description.}
    \label{datasets}
    \begin{tabular}{llll}
    \hline
    ID         & \% of whole dataset & \# of lines & Size (MB) \\
    \hline
    input\_15  & 15                  & 52623       & 11        \\
    input\_30  & 30                  & 105246      & 21        \\
    input\_45  & 45                  & 157872      & 30        \\
    input\_60  & 60                  & 210495      & 40        \\
    input\_75  & 75                  & 263118      & 54        \\
    input\_90  & 90                  & 315741      & 63        \\
    input\_100 & 100                 & 350826      & 69       
    \end{tabular}
\end{table}

The four different test procedures are thought to highlight how the tool behaves from different point of views, especially from the ones of scalability and best/worst case.\\
The \emph{time} it takes to display a result is very important when dealing with this type of systems. Therefore this can be considered as the main measure to state how well such systems perform.\\
Moreoreover a single run for a test cannot be blindly trusted. The same algorithm with the same input may take very different amount of time to output a result due to cold start issues with the Spark cluster, especially when dealing with relatively small datasets. Thus all the charts presented in this section are generated after repeating every test 4 times. The number of repetitions represents a tradeoff between the time taken to run the experiment and the number of runs that would be needed to retrieve statistically significant timing result. Consequently, the average among the 4 identical runs is computed and shown as reference value for each one of them, while the red vertical bars show the difference between the best and the worst performing run.\\

The graph in Figure \ref{scalability} shows how Topic-zoomer scales with respect to the dataset size. The time in the \emph{y} axis is shown in \emph{logarithmic scale}, while the \emph{x} axis represents the percentage of the whole dataset used. It is clear how the time taken by the tool to infer the topics is not exponential in the size of the input data. Thus it is possible to state that Topic-zoomer scales properly and in a linear manner.  
Another important aspect while processing a large amount of data is to show the worst and the best case scenarios for the algorithm. The LDA algorithm is faster when deals with few points located inside the region selected by the user, because the underlying model to be trained is smaller.\\
Figure \ref{square} depicts how performances degrade increasing the size of the region selected. The time in the ordinate axis is always in logarithmic scale to allow a more direct interpretation of the chart. Moreover, in this experiment the grid size (i.e. the \emph{step}) is set to be equal to the one of the selected area \emph{A}. It is not surprising that the time trend in Figure \ref{square} is nearly the same as the one in Figure \ref{scalability}. This means that selecting bigger areas via the top-left and bottom-right corners is the equivalent of having a bigger input dataset.\\  
In addition, Figure \ref{step} shows the performance keeping the selected region unchanged, while the \emph{S} parameter describing the inner grid increases. For this experiment, the selected top-left and bottom-right corners formed a square whose side size is 5. The graph shows a huge growth increasing the grid size from 2 to 3 and from 3 to 4. Moreover, when the step \emph{S} is set to the size of the whole area \emph{A} the time taken to complete the topic extraction seems to decrease. This corroborates the idea that the selection of the \emph{S} parameter highly influences the computation time and that the worst case for the algorithm is when the grid size is set to the one of the side of \emph{A}.\\
Finally, Figure \ref{recomputation} highlights the effect of the method employed to avoid recomputation described in Section \ref{avoidingRecomputation} using the \emph{input\_15} dataset. The time in the \emph{y} axis is always plot in logarithmic scale. As it is possible to see, the time taken to complete the topic extraction decreases as the percentage of the \emph{commonA} area increases. This is clearly due to the smaller amount of points to be considered when training the LDA model. As a result, this method allows Topic-zoomer to run significantly faster even when dealing with rather small input data. 

\begin{figure}[t]
  \center{\includegraphics[width=0.5\textwidth]{scalability.png}}
  \caption{Scalability with different input size.}
  \label{scalability}
\end{figure}
\begin{figure}[t]
  \center{\includegraphics[width=0.5\textwidth]{increasing_square_size.png}}
  \caption{Performance with different area selections.}
  \label{square}
\end{figure}
\begin{figure}[t]
  \center{\includegraphics[width=0.5\textwidth]{increasing_step.png}}
  \caption{Performance with different grid size.}
  \label{step}
\end{figure}
\begin{figure}[t]
  \center{\includegraphics[width=0.5\textwidth]{recomputation.png}}
  \caption{Timings trend avioding recomputation.}
  \label{recomputation}
\end{figure}


\section{Conclusions and future work}
As described in Section \ref{experiments} quite good results are achieved from the scalability point of view. Even though the whole dataset size is not in the order gigabytes, the results are quite promising. Thus it is possible to state that the proposed solution scaled in a quasi-linear manner.\\
Further developments of this work may involve the creation of a graphical user interface (GUI), allowing the users to select the \emph{A} area and all the other parameters in a more convenient way. This can be done implementing a web UI showing a map of the territory we own data about (eg. Google Maps or OpenStreetMap) communicating with the Spark cluster backend. Once succeeded the results can be retrieved and shown to the user either as a table or as a chart.\\
Moreover, some attention should be paid to the visualization of the raw results of the Spark jobs. A neat and well organized presentation can improve not only immediate interpretation of the results, but also the usability of the tool for the general public.\\
Finally, there is room for improvement in the method proposed to avoid useless recomputation. For example it could be implemented using the map-reduce paradigm to further increment the performance from the scalability standpoint. 


%\end{document}  % This is where a 'short' article might terminate
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.

\bibliographystyle{abbrv}
\bibliography{biblio}% biblio.bib is the name of the Bibliography in this case

% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

\end{document}
