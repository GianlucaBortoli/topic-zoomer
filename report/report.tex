\documentclass{sig-alternate-05-2015}
\usepackage{subscript}
\usepackage{tikz}
\usepackage{hyperref}

\begin{document}

\title{Topic-zoomer: a URL categorization system}

\numberofauthors{2}
\author{
    \alignauthor
    Gianluca Bortoli\\
           \affaddr{DISI\,-\,University of Trento}\\
           \affaddr{Student id: 179816}\\
           \email{gianluca.bortoli@studenti.unitn.it}
    \alignauthor
    Pierfrancesco Ardino\\
           \affaddr{DISI\,-\,University of Trento}\\
           \affaddr{Student id: 189159}\\
           \email{pierfrancesco.ardino@studenti.unitn.it}
    }
\maketitle


\begin{abstract}
Smartphones and tablets pervade our lives on a daily basis and people use them in most curious ways.\\ 
The key idea behind this work is to take advantage of the traffic they generate while they are connected to the Internet, extracting \emph{topics} from what people search on the web with their mobile devices. This use case provides also the geographical coordinates of the users based on the telephone cell they are connected to.\\
For the purpose of this work the approximate location of the cell is enough, even thoug it implicitly defines a limit on the precision while selecting an area inside a map.\\
This paper presents Topic-zoomer, a URL categorization system built upon Apache Spark\footnote{\url{http://spark.apache.org/}}.
%\cite{*} % little hack
\end{abstract}


\printccsdesc
\keywords{Big data, Data mining, URL categorization, topic extraction, geolocalized URLs, LDA}


\section{Introduction}
Extracting topics from a text is a very common goal nowadays in Big Data and the problem of categorizing URLs is highly connected to it.\\
Generally speacking, the main aim is to find what a set of texts is talking about. Applying this reasoning to a more specific problem, Topic-zoomer is able to find the topics on a subset of the initial dataset, based on the documents' geographical location.\\
The tool starts from a set of geotagged URLs and is able to restrict its search space only to the web pages lying inside a certain geographical region. In this way it can discover what the pages in that spacific area are talking about.\\
More informally, this work can be employed in many different fields. For example it can be used to see what people are going to do in the future based on their web searches. Moreover it can be useful for public institutions such as municipalities: they could understand the citizens' needs in order to improve the functionality of the city's facilities and their territory coverage.\\
However, the main problem of a real-life usage of this kind of analysis can be the source of the data. ISPs can easily extract and provide geotagged URLs based on the HTTP requests passing through their routers. Furthermore the anonimity of the users generating such traffic must be preserved.\\
This kind of problems is a challange for traditional programming paradigms, since the amount of data can be huge and may also involve straming  processing. Big Data platforms and frameworks perfectly suit this amount of work that cannot fit in a single machine and has to scale as the input size increases. Thus employing an engine for large-scale and distributed data processing is certainly not an option if the tool is thought to be used on real data.        


\section{Related work}
URL categorization is a task that many people and companies are addressing in the last few years. Some of them are even trying to categorize the whole web. Some of the ones involved in this kind of analysis are Cyren\footnote{\url{http://www.cyren.com/url-category-check.html}} and BrightCloud\footnote{\url{https://www.brightcloud.com/tools/change-request-url-categorization.php}}.\\
Moreover, a huge number academic researches have been published and also some patents\footnote{\url{https://www.google.com/patents/US8078625}} have been filed on the subject.\\
However the specific case of geotagged URLs has not raised so much interest neither in the academic nor in the developer communities.


\section{Problem definition}
The task Topic-zoomer aims to solve can be better described showing what the \emph{input} and the \emph{output} are.\\
The \emph{input} is CSV file containing rows of the following form:
\begin{equation}\label{dataset}
    <lat,\,long,\,\{url0\,|url1\,|...\,\}\,>
\end{equation}
The text of the pages pointed by the URLs is analized in order to identify the topics, that can be represented as a vector of words that very often appear togheter.\\
Let \emph{A} be an algorithm to compute such topics (eg. TF/IDF, frequent itemset or LDA), the algorithm \emph{L} gives as a result the topics of the dataset in general.\\
\emph{L} allows the user to restrict the search in the dataset using the following parameters:
\begin{itemize}
    \item \emph{A}: the map area identified by top-left and bottom-right corners. This is the region of interest for retrieving the topics. 
    \item \emph{S}: the size of the grid to be created inside the area \emph{A} selected above (ie. the square side length).
    \item \emph{k}: the number of topics to search inside each square of the grid.
\end{itemize}
In this way \emph{A} is divided into squares of size $S*S$ and the \emph{k} topics are identified inside each such square in the grid.


\section{Solution}
This section describes in detail all the steps taken to realize this tool.
\subsection{Data collection and preprocessing}
The first task was to transform an initial dataset with the form shown in \ref{dataset} into another one of the form
\begin{equation}\label{datasetClean}
    <lat,\,long,\,page\_text\,>
\end{equation}
A \emph{crawler} has been written in order to follow all the links and download the web pages.\\
This may seem a trivial operation at a first glance. However the initial dataset contained a lot of URLs pointing to images, videos, applications' configuration files and many other elements that are far from being a web page. To retrieve only sensible information the crawler rejects all the requests not containing \emph{text/html} in the responses' header. This is the one usually populated by the web server when one of its pages is downloaded. In this way a new CSV file containing rows as described in \ref{datasetClean} is generated.
\subsection{Topic extraction}
After downloading the web pages, the Spark pipeline is ready to run on the clean dataset.
%TODO: describe the pipeline of map|reduce into the `compute` function
\subsection{Avoiding recomputation}
%TODO: describe here the idea about how to avoid recomputing everithing from scratch


\section{Architecture}
Apache Spark and all the other big data distributed frameworks are thought to be set up in a cluster of machines. In this way it is possible to exploit its distributed mode and all the computation power from the worker nodes.\\
To achive this Google Dataproc\footnote{\url{https://cloud.google.com/dataproc/}} on the Google Compute Engine\footnote{\url{https://cloud.google.com/compute/}} (GCE) is used to provision and setup a Spark cluster.\\
One of the main focuses of this work is the \emph{scalability} of this piece of software. To see if Topic-zoomer really succeedes in it, a pseudo-distributed Spark cluster may produce completely misleading or even wrong results. Thus a platform providing highly performing vistual machines (VMs) was mandatory.\\
The cluster used for the experiments described in Section \ref{experiments} is composed as shown in Table \ref{cluster}.
\begin{table}[]
\centering
\caption{GCE cluster node description}
\label{cluster}
\begin{tabular}{lllll}
\hline
Node   & Number & vCPUs & RAM (GB) & SSD (GB) \\
\hline
master & 1      & 1     & 3.75     & 500      \\
worker & 3      & 2     & 7.50     & 500     
\end{tabular}
\end{table}


\section{Experiments}\label{experiments}
%TODO: put here images & descriptions
% remember that time is log scale!
% parameters used for all the experiments (in a table maybe?) 
% table to show the different parts of the dataset with their size in terms of MB and # of lines


\section{Conclusions and future work}
As described in Section \ref{experiments} quite good results are achieved from the scalability point of view. Even though the whole dataset size is not in the order gigabytes, the results are quite promising.


%\end{document}  % This is where a 'short' article might terminate
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.

%\bibliographystyle{abbrv}
%\bibliography{biblio}% sigproc.bib is the name of the Bibliography in this case

% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%

\end{document}
